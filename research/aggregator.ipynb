{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from math import ceil, floor\n",
    "from pprint import pprint\n",
    "from random import random\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"DSP_CACHEBOOL\"] = \"TRUE\"\n",
    "os.environ[\"DSP_CACHEDIR\"] = \"./cache/library\"\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = \"./cache/notebook\"\n",
    "os.environ[\"LITELLM_MODE\"] = \"PRODUCTION\"\n",
    "\n",
    "import dsp\n",
    "import dspy\n",
    "import emoji\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phoenix\n",
    "import psycopg\n",
    "import pydantic\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot\n",
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from openai import OpenAI\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import \\\n",
    "    OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from pgvector.psycopg import register_vector\n",
    "from xid import XID\n",
    "\n",
    "from library.types import *\n",
    "from library.utils import *\n",
    "\n",
    "phoenix.launch_app(host=\"localhost\", port=6006)\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint=\"http://localhost:6006/v1/traces\")))\n",
    "trace_api.set_tracer_provider(tracer_provider)\n",
    "DSPyInstrumentor().instrument()\n",
    "\n",
    "evaluate = Evaluate(devset=None, metric=None, num_threads=os.cpu_count() // 2, display_progress=True, display_table=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres = psycopg.connect(\"host=localhost user=postgres password=postgres dbname=clank\", autocommit=True)\n",
    "register_vector(postgres)\n",
    "pg = postgres.execute\n",
    "\n",
    "te3s = partial(OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]).embeddings.create, model=\"text-embedding-3-small\", encoding_format=\"float\")\n",
    "\n",
    "t5fl = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"./cache/rank-T5-flan\").rerank\n",
    "\n",
    "# TODO: Check and play with STOP sequences\n",
    "params = { \"max_tokens\": 1024, \"temperature\": 0.7 }\n",
    "\n",
    "gpt35 = dspy.ChatBackend(model=\"openai/gpt-3.5-turbo-instruct\", api_key=os.environ[\"OPENAI_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gpt4o = dspy.ChatBackend(model=\"openai/gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gqmix = dspy.ChatBackend(model=\"groq/mixtral-8x7b-32768\", api_key=os.environ[\"GROQ_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gqll3 = dspy.ChatBackend(model=\"groq/llama3-8b-8192\", api_key=os.environ[\"GROQ_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "asmix = dspy.ChatBackend(model=\"anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\", api_key=os.environ[\"ANYSCALE_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "asll3 = dspy.ChatBackend(model=\"anyscale/meta-llama/Meta-Llama-3-8B-Instruct\", api_key=os.environ[\"ANYSCALE_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "dspy.configure(backend=gqll3, trace=[], cache=True) # trace=[] needed to run assertions and suggestions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The current sample has a majority of english feedbacks,\n",
    "#Â this is ok for now but enhance in future iterations\n",
    "with open(\"artifacts/feedbacks/test.json\", \"r\") as file:\n",
    "    feedbacks = json.load(file)\n",
    "\n",
    "feedbacks = pd.DataFrame(feedbacks)\n",
    "feedbacks_with_issues = feedbacks[feedbacks[\"issues\"].apply(len) > 0]\n",
    "feedbacks_with_suggestions = feedbacks[feedbacks[\"suggestions\"].apply(len) > 0]\n",
    "\n",
    "display(feedbacks.head())\n",
    "print(f\"{ceil(feedbacks['content'].apply(len).mean())} average feedback length ~ {ceil(feedbacks['content'].apply(tokenizer).apply(len).mean())} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for issues in feedbacks_with_issues[\"issues\"]:\n",
    "    for issue in issues:\n",
    "        id = XID().string()\n",
    "        text = issue[\"description\"]\n",
    "        embedding = te3s(input=text).data[0].embedding\n",
    "        pg(\"INSERT INTO issue (id, text, embedding) VALUES (%s, %s, %s::vector);\", (id, text, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suggestions in feedbacks_with_suggestions[\"suggestions\"]:\n",
    "    for suggestion in suggestions:\n",
    "        id = XID().string()\n",
    "        text = suggestion[\"description\"]\n",
    "        embedding = te3s(input=text).data[0].embedding\n",
    "        pg(\"INSERT INTO suggestion (id, text, embedding) VALUES (%s, %s, %s::vector);\", (id, text, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = feedbacks_with_issues.iloc[int(random() * len(feedbacks_with_issues))][\"issues\"][0]\n",
    "print(f'{issue[\"title\"]}\\n\\n{issue[\"description\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = te3s(input=issue[\"description\"]).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set runtime parameter hnsw.ef_search = 100\n",
    "similar_issues = pg(\"SELECT text, 1 - (embedding <=> %s::vector) AS score FROM issue ORDER BY score DESC LIMIT 10;\", (embedding,)).fetchall()\n",
    "display(similar_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_issues = [\n",
    "    issue[0] for issue in similar_issues\n",
    "    if float(issue[1]) >= 0.60 and float(issue[1]) < 0.99 # TODO: Enhance all of this and don't do the hacky 0.99 check\n",
    "]\n",
    "display(similar_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_issues = t5fl(RerankRequest(issue[\"description\"], passages=[{\"text\": issue} for issue in similar_issues]))\n",
    "display(similar_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_issues = [\n",
    "    issue[\"text\"] for issue in similar_issues\n",
    "    if float(issue[\"score\"]) >= 0.30\n",
    "][:3]\n",
    "display(similar_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discern by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueSimilarityDiscernor(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        issue: str\n",
    "        options: List[str]\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        index: int\n",
    "\n",
    "    class DiscernSimilarity(dspy.Signature):\n",
    "        \"\"\"\n",
    "Discern whether issue A and issue B, that customers have with a product, are similar or not.\n",
    "- Both issues are similar only if they are at least 80% similar.\n",
    "- Customers can have similar issues without writing them the same way.\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            issue_a: str\n",
    "            issue_b: str\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            similar: bool\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.discern_similarity = ChainOfThought(self.DiscernSimilarity, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "        self.load(\"artifacts/issue_aggregator/issue_similarity_discernor/labeled_few_shot.json\")\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        for index, option in enumerate(input.options):\n",
    "            if option == input.issue:\n",
    "                return dspy.Prediction(output=self.Output(index=index))\n",
    "\n",
    "            similar = self.discern_similarity(input=self.DiscernSimilarity.Input(\n",
    "                issue_a=input.issue,\n",
    "                issue_b=option,\n",
    "            )).output.similar\n",
    "\n",
    "            if similar:\n",
    "                return dspy.Prediction(output=self.Output(index=index))\n",
    "\n",
    "        return dspy.Prediction(output=self.Output(index=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = IssueSimilarityDiscernor()(input=IssueSimilarityDiscernor.Input(\n",
    "    issue=issue[\"description\"],\n",
    "    options=similar_issues,\n",
    ")).output.index\n",
    "similar_issue = similar_issues[index] if index >= 0 else \"\"\n",
    "display(similar_issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_issue = feedbacks_with_issues[feedbacks_with_issues[\"issues\"].apply(lambda issues: issues[0][\"description\"] == similar_issue)][\"issues\"].iloc[0][0]\n",
    "print(f'{similar_issue[\"title\"]}\\n\\n{similar_issue[\"description\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(issue)\n",
    "print(\"=\"*80)\n",
    "pprint(similar_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueMerger(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        class Issue(pydantic.BaseModel):\n",
    "            title: str\n",
    "            description: str\n",
    "            steps: List[str]\n",
    "\n",
    "        issue_a: Issue\n",
    "        issue_b: Issue\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        class Issue(pydantic.BaseModel):\n",
    "            title: str\n",
    "            description: str\n",
    "            steps: List[str]\n",
    "\n",
    "        issue: Issue\n",
    "\n",
    "    class MergeIssues(dspy.Signature):\n",
    "        \"\"\"\n",
    "Merge, coherently, issue A and issue B, that customers have with a product, into a single issue.\n",
    "- Maintain the core problem, context and nuances of both issues.\n",
    "- Do not create information that is not present in any of the issues.\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            class Issue(pydantic.BaseModel):\n",
    "                title: str\n",
    "                description: str\n",
    "                steps: List[str]\n",
    "\n",
    "            issue_a: Issue\n",
    "            issue_b: Issue\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            class Issue(pydantic.BaseModel):\n",
    "                title: str = pydantic.Field(description=\"4 to 10 words, which cannot contain the words `issue` (or synonyms), `customer` (or synonyms) or the product's name.\", max_length=100)\n",
    "                description: str = pydantic.Field(description=\"Long, complete explanation, but without redundant information, using the feedback's original words. Must focus solely on the issue by depersonalizing the sentences.\")\n",
    "                steps: List[str] = pydantic.Field(description=\"Precise steps, but very concise, if any, to be able to reproduce the issue, else `[]`.\", max_items=5)\n",
    "\n",
    "            issue: Issue\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.merge_issues = ChainOfThought(self.MergeIssues, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "        self.load(\"artifacts/issue_aggregator/issue_merger/labeled_few_shot.json\")\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        if input.issue_a == input.issue_b:\n",
    "            return dspy.Prediction(output=self.Output(\n",
    "                issue=self.Output.Issue(\n",
    "                    title=input.issue_a.title,\n",
    "                    description=input.issue_a.description,\n",
    "                    steps=input.issue_a.steps,\n",
    "                ),\n",
    "            ))\n",
    "\n",
    "        issue = self.merge_issues(input=self.MergeIssues.Input(\n",
    "            issue_a=self.MergeIssues.Input.Issue(\n",
    "                title=input.issue_a.title,\n",
    "                description=input.issue_a.description,\n",
    "                steps=input.issue_a.steps,\n",
    "            ),\n",
    "            issue_b=self.MergeIssues.Input.Issue(\n",
    "                title=input.issue_b.title,\n",
    "                description=input.issue_b.description,\n",
    "                steps=input.issue_b.steps,\n",
    "            ),\n",
    "        )).output.issue\n",
    "\n",
    "        dspy.Suggest(\n",
    "            len(issue.steps) <= len(input.issue_a.steps) + len(input.issue_b.steps),\n",
    "            f\"The merged issue's `steps to reproduce` ({len(issue.steps)}) cannot be longer than the sum of the `steps to reproduce` of the original issues ({len(input.issue_a.steps) + len(input.issue_b.steps)})!\"\n",
    "        )\n",
    "\n",
    "        return dspy.Prediction(output=self.Output(\n",
    "            issue=self.Output.Issue(\n",
    "                title=issue.title,\n",
    "                description=issue.description,\n",
    "                steps=issue.steps,\n",
    "            ),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = IssueMerger()(input=IssueMerger.Input(\n",
    "    issue_a=IssueMerger.Input.Issue(\n",
    "        title=issue[\"title\"],\n",
    "        description=issue[\"description\"],\n",
    "        steps=issue[\"steps\"],\n",
    "    ),\n",
    "    issue_b=IssueMerger.Input.Issue(\n",
    "        title=similar_issue[\"title\"],\n",
    "        description=similar_issue[\"description\"],\n",
    "        steps=similar_issue[\"steps\"],\n",
    "    ),\n",
    ")).output.issue\n",
    "pprint(issue.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion = feedbacks_with_suggestions.iloc[int(random() * len(feedbacks_with_suggestions))][\"suggestions\"][0]\n",
    "print(f'{suggestion[\"title\"]}\\n\\n{suggestion[\"description\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = te3s(input=suggestion[\"description\"]).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set runtime parameter hnsw.ef_search = 100\n",
    "similar_suggestions = pg(\"SELECT text, 1 - (embedding <=> %s::vector) AS score FROM suggestion ORDER BY score DESC LIMIT 10;\", (embedding,)).fetchall()\n",
    "display(similar_suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_suggestions = [\n",
    "    suggestion[0] for suggestion in similar_suggestions\n",
    "    if float(suggestion[1]) >= 0.60 and float(suggestion[1]) < 0.99 # TODO: Enhance all of this and don't do the hacky 0.99 check\n",
    "]\n",
    "display(similar_suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_suggestions = t5fl(RerankRequest(suggestion[\"description\"], passages=[{\"text\": suggestion} for suggestion in similar_suggestions]))\n",
    "display(similar_suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_suggestions = [\n",
    "    suggestion[\"text\"] for suggestion in similar_suggestions\n",
    "    if float(suggestion[\"score\"]) >= 0.30\n",
    "][:3]\n",
    "display(similar_suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discern by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuggestionSimilarityDiscernor(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        suggestion: str\n",
    "        options: List[str]\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        index: int\n",
    "\n",
    "    class DiscernSimilarity(dspy.Signature):\n",
    "        \"\"\"\n",
    "Discern whether suggestion A and suggestion B, that customers have about a product, are similar or not.\n",
    "- Both suggestions are similar only if they are at least 80% similar.\n",
    "- Customers can have similar suggestions without writing them the same way.\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            suggestion_a: str\n",
    "            suggestion_b: str\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            similar: bool\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.discern_similarity = ChainOfThought(self.DiscernSimilarity, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "        self.load(\"artifacts/suggestion_aggregator/suggestion_similarity_discernor/labeled_few_shot.json\")\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        for index, option in enumerate(input.options):\n",
    "            if option == input.suggestion:\n",
    "                return dspy.Prediction(output=self.Output(index=index))\n",
    "\n",
    "            similar = self.discern_similarity(input=self.DiscernSimilarity.Input(\n",
    "                suggestion_a=input.suggestion,\n",
    "                suggestion_b=option,\n",
    "            )).output.similar\n",
    "\n",
    "            if similar:\n",
    "                return dspy.Prediction(output=self.Output(index=index))\n",
    "\n",
    "        return dspy.Prediction(output=self.Output(index=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SuggestionSimilarityDiscernor()(input=SuggestionSimilarityDiscernor.Input(\n",
    "    suggestion=suggestion[\"description\"],\n",
    "    options=similar_suggestions,\n",
    ")).output.index\n",
    "similar_suggestion = similar_suggestions[index] if index >= 0 else \"\"\n",
    "display(similar_suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_suggestion = feedbacks_with_suggestions[feedbacks_with_suggestions[\"suggestions\"].apply(lambda suggestions: suggestions[0][\"description\"] == similar_suggestion)][\"suggestions\"].iloc[0][0]\n",
    "print(f'{similar_suggestion[\"title\"]}\\n\\n{similar_suggestion[\"description\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(suggestion)\n",
    "print(\"=\"*80)\n",
    "pprint(similar_suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuggestionMerger(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        class Suggestion(pydantic.BaseModel):\n",
    "            title: str\n",
    "            description: str\n",
    "            reason: str\n",
    "\n",
    "        suggestion_a: Suggestion\n",
    "        suggestion_b: Suggestion\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        class Suggestion(pydantic.BaseModel):\n",
    "            title: str\n",
    "            description: str\n",
    "            reason: str\n",
    "\n",
    "        suggestion: Suggestion\n",
    "\n",
    "    class MergeSuggestions(dspy.Signature):\n",
    "        \"\"\"\n",
    "Merge, coherently, suggestion A and suggestion B, that customers have about a product, into a single suggestion.\n",
    "- Maintain the core idea, context and nuances of both suggestions.\n",
    "- Do not create information that is not present in any of the suggestions.\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            class Suggestion(pydantic.BaseModel):\n",
    "                title: str\n",
    "                description: str\n",
    "                reason: str\n",
    "\n",
    "            suggestion_a: Suggestion\n",
    "            suggestion_b: Suggestion\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            class Suggestion(pydantic.BaseModel):\n",
    "                title: str = pydantic.Field(description=\"4 to 10 words, which cannot contain the words `suggestion` (or synonyms), `customer` (or synonyms) or the product's name.\", max_length=100)\n",
    "                description: str = pydantic.Field(description=\"Long, complete explanation, but without redundant information, using the feedback's original words. Must focus solely on the suggestion by depersonalizing the sentences.\")\n",
    "                reason: str = pydantic.Field(description=f'The customer\\'s motivation behind the proposal of the suggestion, if any must always start with `This will`, else `{UNKNOWN_OPTION}`.')\n",
    "\n",
    "            suggestion: Suggestion\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.merge_suggestions = ChainOfThought(self.MergeSuggestions, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "        self.load(\"artifacts/suggestion_aggregator/suggestion_merger/labeled_few_shot.json\")\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        if input.suggestion_a == input.suggestion_b:\n",
    "            return dspy.Prediction(output=self.Output(\n",
    "                suggestion=self.Output.Suggestion(\n",
    "                    title=input.suggestion_a.title,\n",
    "                    description=input.suggestion_a.description,\n",
    "                    reason=input.suggestion_a.reason,\n",
    "                ),\n",
    "            ))\n",
    "\n",
    "        suggestion = self.merge_suggestions(input=self.MergeSuggestions.Input(\n",
    "            suggestion_a=self.MergeSuggestions.Input.Suggestion(\n",
    "                title=input.suggestion_a.title,\n",
    "                description=input.suggestion_a.description,\n",
    "                reason=input.suggestion_a.reason,\n",
    "            ),\n",
    "            suggestion_b=self.MergeSuggestions.Input.Suggestion(\n",
    "                title=input.suggestion_b.title,\n",
    "                description=input.suggestion_b.description,\n",
    "                reason=input.suggestion_b.reason,\n",
    "            ),\n",
    "        )).output.suggestion\n",
    " \n",
    "        return dspy.Prediction(output=self.Output(\n",
    "            suggestion=self.Output.Suggestion(\n",
    "                title=suggestion.title,\n",
    "                description=suggestion.description,\n",
    "                reason=suggestion.reason if suggestion.reason.upper() != UNKNOWN_OPTION else \"\",\n",
    "            ),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion = SuggestionMerger()(input=SuggestionMerger.Input(\n",
    "    suggestion_a=SuggestionMerger.Input.Suggestion(\n",
    "        title=suggestion[\"title\"],\n",
    "        description=suggestion[\"description\"],\n",
    "        reason=suggestion[\"reason\"],\n",
    "    ),\n",
    "    suggestion_b=SuggestionMerger.Input.Suggestion(\n",
    "        title=similar_suggestion[\"title\"],\n",
    "        description=similar_suggestion[\"description\"],\n",
    "        reason=similar_suggestion[\"reason\"],\n",
    "    ),\n",
    ")).output.suggestion\n",
    "pprint(suggestion.model_dump())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-Lu3HgufU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
