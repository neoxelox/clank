{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from math import ceil, floor\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"DSP_CACHEBOOL\"] = \"TRUE\"\n",
    "os.environ[\"DSP_CACHEDIR\"] = \"./cache/library\"\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = \"./cache/notebook\"\n",
    "os.environ[\"LITELLM_MODE\"] = \"PRODUCTION\"\n",
    "\n",
    "import dsp\n",
    "import dspy\n",
    "import emoji\n",
    "import Levenshtein\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phoenix\n",
    "import pydantic\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot\n",
    "from dspy.teleprompt.signature_opt_typed import optimize_signature\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import \\\n",
    "    OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "from library.types import *\n",
    "from library.utils import *\n",
    "\n",
    "phoenix.launch_app(host=\"localhost\", port=6006)\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint=\"http://localhost:6006/v1/traces\")))\n",
    "trace_api.set_tracer_provider(tracer_provider)\n",
    "DSPyInstrumentor().instrument()\n",
    "\n",
    "evaluate = Evaluate(devset=None, metric=None, num_threads=os.cpu_count() // 2, display_progress=True, display_table=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check and play with STOP sequences\n",
    "params = { \"max_tokens\": 1024, \"temperature\": 0.7 }\n",
    "\n",
    "gpt35 = dspy.ChatBackend(model=\"openai/gpt-3.5-turbo-instruct\", api_key=os.environ[\"OPENAI_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gpt4o = dspy.ChatBackend(model=\"openai/gpt-4o\", api_key=os.environ[\"OPENAI_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gqmix = dspy.ChatBackend(model=\"groq/mixtral-8x7b-32768\", api_key=os.environ[\"GROQ_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "gqll3 = dspy.ChatBackend(model=\"groq/llama3-8b-8192\", api_key=os.environ[\"GROQ_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "asmix = dspy.ChatBackend(model=\"anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\", api_key=os.environ[\"ANYSCALE_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "asll3 = dspy.ChatBackend(model=\"anyscale/meta-llama/Meta-Llama-3-8B-Instruct\", api_key=os.environ[\"ANYSCALE_API_KEY\"], params=params, attempts=3, system_prompt=SYSTEM_PROMPT)\n",
    "\n",
    "dspy.configure(backend=gqll3, trace=[], cache=True) # trace=[] needed to run assertions and suggestions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The current sample has a majority of english feedbacks,\n",
    "#Â this is ok for now but enhance in future iterations\n",
    "with open(\"artifacts/feedbacks/labeled.json\", \"r\") as file:\n",
    "    feedbacks = json.load(file)\n",
    "\n",
    "feedbacks = pd.DataFrame(feedbacks)\n",
    "display(feedbacks.head())\n",
    "print(f\"{ceil(feedbacks['content'].apply(len).mean())} average feedback length ~ {ceil(feedbacks['content'].apply(tokenizer).apply(len).mean())} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDetector(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        feedback: str\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        language: str\n",
    "\n",
    "    class DetectLanguage(dspy.Signature):\n",
    "        \"\"\"\n",
    "Detect the language from the customer's feedback.\n",
    "- If the words are common in many languages including English, default to English.\n",
    "- If there are lexicographic, syntactic, spelling, grammar or any other language mistakes, default to the most probable language.\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            feedback: str\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            language: str = pydantic.Field(description=f\"The full name of the valid language that best fits, if any, else `{UNKNOWN_OPTION}`.\")\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.LANGUAGES = [language.name.upper() for language in Language.all()]\n",
    "        self.DEFAULT_LANGUAGE = \"ENGLISH\"\n",
    "        self.MINIMUM_LENGTH = 1\n",
    "        self.MINIMUM_CONFIDENCE = 0.25\n",
    "        self.MINIMUM_CONFIDENCE_DISTANCE = self.MINIMUM_CONFIDENCE / 2\n",
    "\n",
    "        self.detector = LanguageDetectorBuilder.from_languages(\n",
    "            Language.ENGLISH,\n",
    "            Language.SPANISH,\n",
    "            Language.FRENCH,\n",
    "            Language.PORTUGUESE,\n",
    "            Language.GERMAN,\n",
    "            Language.ITALIAN,\n",
    "        ).with_preloaded_language_models().build()\n",
    "\n",
    "        self.detect_language = ChainOfThought(self.DetectLanguage, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        feedback = emoji.replace_emoji(input.feedback)\n",
    "\n",
    "        if len(feedback.strip()) < self.MINIMUM_LENGTH:\n",
    "            return dspy.Prediction(output=self.Output(\n",
    "                language=self.DEFAULT_LANGUAGE,\n",
    "            ))\n",
    "\n",
    "        confidence_values = self.detector.compute_language_confidence_values(feedback)\n",
    "        most_likely = confidence_values[0]\n",
    "        second_most_likely = confidence_values[1]\n",
    "\n",
    "        if (\n",
    "            most_likely.value < self.MINIMUM_CONFIDENCE\n",
    "            or (most_likely.value - second_most_likely.value) < self.MINIMUM_CONFIDENCE_DISTANCE\n",
    "        ):\n",
    "            language = self.detect_language(input=self.DetectLanguage.Input(\n",
    "                feedback=feedback,\n",
    "            )).output.language\n",
    "\n",
    "            language = language.upper()\n",
    "\n",
    "            dspy.Suggest(\n",
    "                language in self.LANGUAGES or language == UNKNOWN_OPTION,\n",
    "                f'Language must be {self.DetectLanguage.Output.model_fields[\"language\"].description}! `{language}` is NOT a valid language. Valid languages are:\\n' + \"\".join([f\"- {option}\\n\" for option in self.LANGUAGES])\n",
    "            )\n",
    "\n",
    "            if language not in self.LANGUAGES:\n",
    "                language = UNKNOWN_OPTION\n",
    "\n",
    "            return dspy.Prediction(output=self.Output(\n",
    "                language=language,\n",
    "            ))\n",
    "\n",
    "        return dspy.Prediction(output=self.Output(\n",
    "            language=most_likely.language.name.upper(),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = [\n",
    "    dspy.Example(\n",
    "        input=LanguageDetector.Input(\n",
    "            feedback=feedback[\"content\"],\n",
    "        ),\n",
    "        # TODO: Needs reasoning output field\n",
    "        output=LanguageDetector.Output(\n",
    "            language=feedback[\"language\"],\n",
    "        ),\n",
    "    ).with_inputs(\"input\")\n",
    "    for _, feedback in feedbacks.iterrows()\n",
    "]\n",
    "trainset = fullset[:floor(len(fullset) * 0.7)]\n",
    "testset = fullset[len(trainset):len(trainset)+floor(len(fullset) * 0.2)]\n",
    "devset = fullset[len(trainset)+len(testset):]\n",
    "print(f\"trainset({len(trainset)}) + testset({len(testset)}) + devset({len(devset)}) = {len(fullset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detector_metric(label: dspy.Example, prediction: dspy.Prediction, trace: list = []) -> float:\n",
    "    score = 0\n",
    "\n",
    "    # [100%] Detected language is equal to the labeled language\n",
    "    if prediction.output.language == label.output.language:\n",
    "        score += 100\n",
    "\n",
    "    return round(score / 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "zs_language_detector = LanguageDetector()\n",
    "zs_language_detector.load(\"artifacts/language_detector/zero_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and save artifact\n",
    "zs_language_detector = LanguageDetector()\n",
    "zs_language_detector.save(\"artifacts/language_detector/zero_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(zs_language_detector, metric=language_detector_metric, devset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "lfs_language_detector = LanguageDetector()\n",
    "lfs_language_detector.load(\"artifacts/language_detector/labeled_few_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and save artifact\n",
    "lfs_language_detector = LabeledFewShot(\n",
    "    k=4\n",
    ").compile(\n",
    "    LanguageDetector(),\n",
    "    trainset=trainset,\n",
    ")\n",
    "lfs_language_detector.save(\"artifacts/language_detector/labeled_few_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lfs_language_detector, metric=language_detector_metric, devset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackTranslator(dspy.Module):\n",
    "    class Input(pydantic.BaseModel):\n",
    "        feedback: str\n",
    "        from_language: str\n",
    "        to_language: str\n",
    "\n",
    "    class Output(pydantic.BaseModel):\n",
    "        translation: str\n",
    "\n",
    "    class TranslateFeedback(dspy.Signature):\n",
    "        \"\"\"\n",
    "Translate the customer's feedback from a language to a language.\n",
    "Maintain the feedback's:\n",
    "- Style\n",
    "- Format (including newlines and tabs)\n",
    "- Emphasis\n",
    "- Emojis\n",
    "- Punctuation\n",
    "- Names\n",
    "- Measures\n",
    "- Units\n",
    "- Dates (use the translated format)\n",
    "        \"\"\"\n",
    "\n",
    "        class Input(pydantic.BaseModel):\n",
    "            feedback: str\n",
    "            from_language: str\n",
    "            to_language: str\n",
    "\n",
    "        class Output(pydantic.BaseModel):\n",
    "            translation: str\n",
    "\n",
    "        input: Input = dspy.InputField()\n",
    "        output: Output = dspy.OutputField()\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.MAXIMUM_DIFFERENCE = 50\n",
    "\n",
    "        self.translate_feedback = ChainOfThought(self.TranslateFeedback, max_retries=3, explain_errors=False)\n",
    "\n",
    "        self.activate_assertions(handler=dspy.backtrack_handler, max_backtracks=3)\n",
    "\n",
    "    def forward(self, input: Input) -> dspy.Prediction:\n",
    "        if input.from_language == input.to_language:\n",
    "            return dspy.Prediction(output=self.Output(\n",
    "                translation=input.feedback,\n",
    "            ))\n",
    "\n",
    "        translation = self.translate_feedback(input=self.TranslateFeedback.Input(\n",
    "            feedback=input.feedback,\n",
    "            from_language=input.from_language,\n",
    "            to_language=input.to_language,\n",
    "        )).output.translation\n",
    "\n",
    "        dspy.Assert(translation != \"\", 'Translation cannot be empty!')\n",
    "\n",
    "        return dspy.Prediction(output=self.Output(\n",
    "            translation=translation,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = feedbacks[(feedbacks[\"language\"] != \"ENGLISH\") & (feedbacks[\"language\"] != UNKNOWN_OPTION)]\n",
    "fullset = [\n",
    "    dspy.Example(\n",
    "        input=FeedbackTranslator.Input(\n",
    "            feedback=feedback[\"content\"],\n",
    "            from_language=feedback[\"language\"],\n",
    "            to_language=\"ENGLISH\",\n",
    "        ),\n",
    "        # TODO: Needs reasoning output field\n",
    "        output=FeedbackTranslator.Output(\n",
    "            translation=feedback[\"translation\"],\n",
    "        ),\n",
    "    ).with_inputs(\"input\")\n",
    "    for _, feedback in fullset.iterrows()\n",
    "]\n",
    "trainset = fullset[:floor(len(fullset) * 0.7)]\n",
    "testset = fullset[len(trainset):len(trainset)+floor(len(fullset) * 0.2)]\n",
    "devset = fullset[len(trainset)+len(testset):]\n",
    "print(f\"trainset({len(trainset)}) + testset({len(testset)}) + devset({len(devset)}) = {len(fullset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssessTranslationQuality(dspy.Signature):\n",
    "    \"\"\"\n",
    "Assess the quality of a feedback translation taking into account the original instructions.\n",
    "    \"\"\"\n",
    "\n",
    "    feedback = dspy.InputField()\n",
    "    language = dspy.InputField(format=str.capitalize)\n",
    "    instructions_ = dspy.InputField(prefix=\"Instructions:\", format=lambda i: f'\"\"\"\\n{i[i.find(\"---\")+3:].strip() if \"---\" in i else i}\\n\"\"\"')\n",
    "    translation = dspy.InputField()\n",
    "    quality = dspy.OutputField(desc=\"integer number from 0 (low quality) to 50 (high quality)\")\n",
    "\n",
    "\n",
    "def feedback_translator_metric(label: dspy.Example, prediction: dspy.Prediction, trace: list = []) -> float:\n",
    "    score = 0\n",
    "\n",
    "    # [20%] Translation length must be maximum 30% different from the original feedback\n",
    "    length_difference = min(max(abs(1 - len(prediction.output.translation)/len(label.input.feedback)) * 100, 0), 100)\n",
    "\n",
    "    if length_difference <= 30:\n",
    "        score += map_range(length_difference, 0, 30, 20, 0)\n",
    "\n",
    "    # [5%] Translation must have the same emojis as in the original feedback\n",
    "    prediction_emojis = [item[\"emoji\"] for item in emoji.emoji_list(prediction.output.translation)]\n",
    "    label_emojis = [item[\"emoji\"] for item in emoji.emoji_list(label.output.translation)]\n",
    "\n",
    "    if prediction_emojis == label_emojis:\n",
    "        score += 5\n",
    "\n",
    "    # [25%] Translation must be maximum 20% different from the labeled translation\n",
    "    # TODO: Enhance, probably it's better to do bi-encoding with cosine similarity or cross-encoding score classification\n",
    "    translation_difference = (1 - Levenshtein.ratio(prediction.output.translation, label.output.translation)) * 100\n",
    "\n",
    "    if translation_difference <= 20:\n",
    "        score += map_range(translation_difference, 0, 20, 25, 0)\n",
    "\n",
    "    # [50%] GPT-4o translation quality assesment taking into account the translation instructions\n",
    "    # TODO: Enhance, this is very expensive and does not provide too much help because GPT-4 is\n",
    "    #       very permissive!\n",
    "    # with dspy.context(lm=gpt4o):\n",
    "    #     quality = dspy.Predict(AssessTranslationQuality)(\n",
    "    #         feedback=label.input.feedback,\n",
    "    #         language=label.input.language,\n",
    "    #         instructions_=FeedbackTranslator.TranslateFeedback.__doc__,\n",
    "    #         translation=prediction.output.translation,\n",
    "    #     ).quality\n",
    "\n",
    "    #     try:\n",
    "    #         # GPT-4 sometimes over-explains\n",
    "    #         for i, c in enumerate(quality):\n",
    "    #             if c.isdigit():\n",
    "    #                 break\n",
    "    #         quality = min(max(int(quality[i:i+2]), 0), 50)\n",
    "    #     except Exception:\n",
    "    #         quality = 0\n",
    "\n",
    "    #     score += quality\n",
    "    score += 50\n",
    "\n",
    "    return round(score / 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "zs_feedback_translator = FeedbackTranslator()\n",
    "zs_feedback_translator.load(\"artifacts/feedback_translator/zero_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and save artifact\n",
    "zs_feedback_translator = FeedbackTranslator()\n",
    "zs_feedback_translator.save(\"artifacts/feedback_translator/zero_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(zs_feedback_translator, metric=feedback_translator_metric, devset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "lfs_feedback_translator = FeedbackTranslator()\n",
    "lfs_feedback_translator.load(\"artifacts/feedback_translator/labeled_few_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and save artifact\n",
    "lfs_feedback_translator = LabeledFewShot(\n",
    "    k=4\n",
    ").compile(\n",
    "    FeedbackTranslator(),\n",
    "    trainset=trainset,\n",
    ")\n",
    "lfs_feedback_translator.save(\"artifacts/feedback_translator/labeled_few_shot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lfs_feedback_translator, metric=feedback_translator_metric, devset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Few Shot with Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "bfs_feedback_translator = FeedbackTranslator()\n",
    "bfs_feedback_translator.load(\"artifacts/feedback_translator/bootstrap_few_shot_with_random_search.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and save artifact\n",
    "bfs_feedback_translator = BootstrapFewShotWithRandomSearch(\n",
    "    metric=feedback_translator_metric,\n",
    "    metric_threshold=0.75,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=8,\n",
    "    num_candidate_programs=6,\n",
    "    max_rounds=1,\n",
    "    num_threads=os.cpu_count() // 2,\n",
    "    max_errors=len(trainset) // 2,\n",
    ").compile(\n",
    "    FeedbackTranslator(),\n",
    "    teacher=None,\n",
    "    trainset=trainset,\n",
    ")\n",
    "bfs_feedback_translator.save(\"artifacts/feedback_translator/bootstrap_few_shot_with_random_search.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(bfs_feedback_translator, metric=feedback_translator_metric, devset=testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved artifact\n",
    "so_feedback_translator = FeedbackTranslator()\n",
    "so_feedback_translator.load(\"artifacts/feedback_translator/signature_optimizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WATCH OUT! THIS IS VERY EXPENSIVE!! USES GPT-4o FOR PROMPT GENERATION AND IT IS VERY EXPENSIVE!!!\n",
    "# Compile and save artifact\n",
    "so_feedback_translator = optimize_signature(\n",
    "    FeedbackTranslator(),\n",
    "    evaluator=Evaluate(\n",
    "        metric=feedback_translator_metric,\n",
    "        devset=trainset,\n",
    "        num_threads=os.cpu_count() // 2,\n",
    "        display_progress=True,\n",
    "        display_table=False,\n",
    "    ),\n",
    "    prompt_model=gpt4o,\n",
    "    n_iterations=6,\n",
    "    max_examples=8,\n",
    "    initial_prompts=5,\n",
    ").program\n",
    "so_feedback_translator.save(\"artifacts/feedback_translator/signature_optimizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(so_feedback_translator, metric=feedback_translator_metric, devset=testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-Lu3HgufU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
